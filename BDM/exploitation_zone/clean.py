from pyspark.sql import SparkSession
from pyspark.conf import SparkConf
from monetdb_reader import DBReader
from pathlib import Path
from neo4j_connector import Neo4jConnector
from pyspark.sql.functions import *

import os
import sys

sys.path.insert(0, os.path.dirname(Path(__file__).parent.absolute()))
import configs.common as common


graphdb = Neo4jConnector()

def preload_cleaning(): ## check if product exists before creating
        
        conn = graphdb.get_conn()
        with conn.session() as session:

            ## Delete the first subgraph
            result = session.run("""
                CALL gds.graph.list( "ProductsBoughtTogether" ) YIELD *
            """)
            
            if result.single()!=None:
                 session.run("""
                    CALL gds.graph.drop( "ProductsBoughtTogether" )
                """) 

            ## Delete the community subgraph   
            result = session.run("""
                CALL gds.graph.list( "ProductCommunities" ) YIELD *
            """)

            if result.single()!=None:
                 session.run("""
                    CALL gds.graph.drop( "ProductCommunities" )
                """) 
                 
            session.run("""
                    MATCH (:Product)-[r:in_stock]-(s:Shipment)
                    DELETE r,s
                """)    

        graphdb.close_conn()

preload_cleaning();